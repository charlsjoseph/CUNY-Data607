---
title: "Data 607 Project-4"
author: "Charls Joseph"
date: "November 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(RCurl)
library(XML)

# # Set path of Rtools
# Sys.setenv(PATH = paste(Sys.getenv("PATH"), "*InstallDirectory*/Rtools/bin/",
#                        "*InstallDirectory*/Rtools/mingw_64/bin", sep = ";")) #for 64 bit version
# Sys.setenv(BINPREF = "*InstallDirectory*/Rtools/mingw_64/bin")
# library(devtools)
# 
# #Manually "force" version to be accepted 
# assignInNamespace("version_info", c(devtools:::version_info, list("3.5" = list(version_min = "3.3.0", version_max = "99.99.99", path = "bin"))), "devtools")
# find_rtools() # is TRUE now
# 



# 
# devtools::install_github("hrbrmstr/msgxtractr")
# library(msgxtractr)

```


## 
Read all the spam email contents from dump and prepare a dataframe with column `msg` as the message content 
and the `class` as an indicator whether the message is spam or ham 

Class = 1 => SPAM
Class = 0 => HAM

```{r}
dir="C:/Users/Charls/Documents/CunyMSDS/607-Data Acquiction/project-4/dataset/spam/"
filename = list.files(dir)

msgContent<-NA
for(i in 1:length(filename))
{
  filepath<-paste0(dir,filename[1])  
  text <-readLines(filepath)
  
  # Extract the body of the message. 
  # if the body of the message is an html, extract the text from the html, else extract only the text. 
  # text <- htmlToText(text)
  eachMsg<- list(paste(text, collapse="\n"))
  msgContent = c(msgContent,eachMsg)
  
}

spam<-data.frame()
spam<-as.data.frame(unlist(msgContent),stringsAsFactors = FALSE)
spam$class<-1
colnames(spam)<-c("msg","class")

```

Total number of observation for 'SPAM' emails: 

```{r}
nrow(spam)

```

Read Ham emails in the similar way. 

```{r}

dir="C:/Users/Charls/Documents/CunyMSDS/607-Data Acquiction/project-4/dataset/easy_ham/"
filename = list.files(dir)

msgContent<-NA
for(i in 1:length(filename))
{
  filepath<-paste0(dir,filename[1])  
  text <-readLines(filepath)
  
  # Extract the body of the message. 
  # if the body of the message is an html, extract the text from the html, else extract only the text. 
  # text <- htmlToText(text)
  eachMsg<- list(paste(text, collapse="\n"))
  msgContent = c(msgContent,eachMsg)
  
}

ham<-data.frame()
ham<-as.data.frame(unlist(msgContent),stringsAsFactors = FALSE)
ham$class<-0
colnames(ham)<-c("msg","class")



```
Total number of observation for 'HAM' emails: 

```{r}
nrow(ham)
```

Merge the 'HAM' and 'SPAM' dataset into a final dataframe. 


```{r}
mergeDataSet<-rbind(spam,ham)

nrow(mergeDataSet)
```
## Creating Corpus and cleaning the message text
1. Convert into lower.
2. removing numbers
3. remove Punctuation.
4. remove stop words using snowballC package.
5. Stemming the words into its root word. 
6. stripping off the whit space created by the removal of words, numbers. 

```{r}
#install.packages(c("tm", "SnowballC"))
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(mergeDataSet$msg))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
```
### Creating the Bag of Words model
```{r}
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$class = mergeDataSet$class

head(dataset$class)
nrow(dataset)
```

### Shuffling the dataset. 

Also we need to shuffle the dataset before buildin the machine learning model. 

```{r}
shuffledDF <- dataset[sample(1:nrow(dataset)),]
nrow(shuffledDF)
```


### **Spliting** the dataset into training and test data set : 

Split the complete data set into training set and test set. 

```{r}

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(shuffledDF$class, SplitRatio = 0.8)
training_set = subset(shuffledDF, split == TRUE)
test_set = subset(shuffledDF, split == FALSE)


```

Total number of training set observations: 

```{r}
nrow(training_set)
```

Total number of test set observations: 

```{r}

nrow(test_set)
```

The total number of observations in the bag of word matrix. 

```{r}
no_observation <- ncol(training_set) - 1
no_observation
```


### Modeling using Random forest. 

Fitting Random Forest Classification to the Training set 

```{r}

# Fitting Random Forest Classification to the Training set
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-no_observation],
                          y = training_set$class,
                          ntree = 10)

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-no_observation])

nrow(subset(test_set, class == 0))

# Making the Confusion Matrix
cm = table(test_set[, no_observation], y_pred)
cm
```

