---
title: "Data 607 Project-4"
author: "Charls Joseph"
date: "November 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Commented out the library installation lines.
#library(readtext)
#library(dplyr)
# install.packages("readtext")
#install.packages("wordcloud")
#install.packages("RODBC")

library(RODBC)
library(wordcloud)
library(dplyr)
library(readtext)
library(stringr)

```

## Project 4 - Email Classifer

This Project is to build a predictive classifer to classify an email a spam or ham. A machine Learning model needs to be build from some of the dataset posted in https://spamassassin.apache.org/old/publiccorpus/ and the model would be used to predict the new dataset whether it is a spam and ham.

## Reading Spam and Ham email content
Lets read spam emails from the local directory. 
Spam emails are downloaded from 20030228_spam.tar.bz2 in https://spamassassin.apache.org/old/publiccorpus/

Read all the spam email contents from dump and prepare a dataframe with column `msg` as the message content 
and the `class` as an indicator whether the message is spam or ham 

Class = 1 => SPAM
Class = 0 => HAM

Define a function to extract the message body from the email content. Because we dont need to include all the words which comes in the header. 

```{r}
# define a function to extract the message body from the email content. 

get_email_body <- function(emailContent){
  msge <- str_split(emailContent,"\n\n") %>% unlist()
  body <- paste(msge[2:length(msge)], collapse=' ' )
  return(body)
}
```

Reading the spam emails 

```{r}
dir="C:/Users/Charls/Documents/CunyMSDS/607-Data Acquiction/project-4/dataset/spam/"
filename = list.files(dir)

msgContent<-NA
for(i in 1:length(filename))
{
  filepath<-paste0(dir,filename[i])  
  emailContent <-suppressWarnings(warning(readtext(filepath)))
  msg <- get_email_body(emailContent)
  msg <- gsub("<.*?>", " ", msg)
  eachMsg<- list(paste(msg, collapse="\n"))
  msgContent = c(msgContent,eachMsg)
  
}
spam<-data.frame()
spam<-as.data.frame(unlist(msgContent),stringsAsFactors = FALSE)
spam$class<-1
colnames(spam)<-c("msg","class")


```

Total number of observation for 'SPAM' emails: 

```{r}
nrow(spam)

```

Read Ham emails in the similar way. 

```{r}

dir="C:/Users/Charls/Documents/CunyMSDS/607-Data Acquiction/project-4/dataset/easy_ham/"
filename = list.files(dir)

msgContent<-NA
for(i in 1:length(filename))
{
  filepath<-paste0(dir,filename[i])  
  emailContent <-suppressWarnings(warning(readtext(filepath)))
  msg <- get_email_body(emailContent)
  msg <- gsub("<.*?>", " ", msg)
  eachMsg<- list(paste(msg, collapse="\n"))
  msgContent = c(msgContent,eachMsg)
  
}

ham<-data.frame()
ham<-as.data.frame(unlist(msgContent),stringsAsFactors = FALSE)
ham$class<-0
colnames(ham)<-c("msg","class")

```
Total number of observation for 'HAM' emails: 

```{r}
nrow(ham)
```

Merge the 'HAM' and 'SPAM' dataset into a final dataframe. 


```{r}
mergeDataSet<-rbind(spam,ham)
nrow(mergeDataSet)
```

## Creating Corpus and cleaning the message text

1. Convert into lower.
2. removing numbers
3. remove Punctuation.
4. remove stop words using snowballC package.
5. Stemming the words into its root word. 
6. stripping off the whit space created by the removal of words, numbers. 

```{r}
#install.packages(c("tm", "SnowballC"))
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(mergeDataSet$msg))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
```


### Creating the Bag of Words model
```{r}
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
# removing the most sparse terms
dtm = removeSparseTerms(dtm, 0.98)
dataset = as.data.frame(as.matrix(dtm))
dataset$outputType = mergeDataSet$class

spamDF <- dataset %>% filter(`outputType` == "1" )
nrow(spamDF)
hamDF <- dataset %>% filter(`outputType` == "0" )
nrow(hamDF)

head(dataset$outputType)
nrow(dataset)
```

### word cloud for SPAM emails.

This is the most frequent words in the spam mail dataset. You can see the word 'free', 'money' and the words with spelling mistakes coming up. 

```{r}

spam_word_frequency <- colSums(spamDF)
spam_word_frequency <- sort(spam_word_frequency, decreasing = TRUE)
spam_word_frequency[1:20]

spam_words <- names(spam_word_frequency)
wordcloud(spam_words[1:50], spam_word_frequency[1:50])

```

### word cloud for Ham emails.

This is the most frequent words in the spam mail dataset. 

```{r}
ham_word_frequency <- colSums(hamDF)
ham_word_frequency <- sort(ham_word_frequency, decreasing = TRUE)
ham_word_frequency[1:20]

ham_words <- names(ham_word_frequency)
wordcloud(ham_words[1:50], ham_word_frequency[1:50])
```


### Shuffling the dataset

Also we need to shuffle the dataset before buildin the machine learning model. 

```{r}
shuffledDF <- dataset[sample(1:nrow(dataset)),]
nrow(shuffledDF)

```


### Spliting the dataset into training and test data set 

Split the complete data set into training set(80%) and test set(20%)

```{r}

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(shuffledDF$outputType, SplitRatio = 0.8)
training_set = subset(shuffledDF, split == TRUE)
test_set = subset(shuffledDF, split == FALSE)


```

Total number of training set observations: 

```{r}
nrow(training_set)

```

Total number of test set observations: 

```{r}

nrow(test_set)
```

The total number of observations in the bag of word matrix. 

```{r}
no_observation <- ncol(training_set) - 1
no_observation
```


### Modeling using Random forest. 

Fitting Random Forest Classification to the Training set 

```{r}

# Fitting Random Forest Classification to the Training set
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-no_observation],
                          y = training_set$outputType,
                          ntree = 3)


```

### Predicting the new dataset. 

Here we are using the test data. 

```{r}
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-no_observation])


# Making the Confusion Matrix

cm <- table(y_pred>0,test_set$outputType)
cm
```

The accuracy of the model is 


```{r}
success <- cm['TRUE', 2] + cm['FALSE', 1] 
accuracy <- success/nrow(test_set) * 100
accuracy

```
